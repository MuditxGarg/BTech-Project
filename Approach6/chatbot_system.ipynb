{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mteb.evaluation.evaluators import RetrievalEvaluator\n",
    "from colpali_engine.utils.torch_utils import get_torch_device\n",
    "from mteb.evaluation.evaluators.RetrievalEvaluator import DenseRetrievalExactSearch, DRESModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class MyEvaluatorWrapper:\n",
    "    def __init__(self, is_multi_vector=False, retriever_model_name='all-MiniLM-L6-v2'):\n",
    "        self.is_multi_vector = is_multi_vector\n",
    "        self.device = get_torch_device()\n",
    "\n",
    "        # Load a compatible retriever model\n",
    "        retriever_model = SentenceTransformer(retriever_model_name)\n",
    "        \n",
    "        # Define retriever as an instance of DenseRetrievalExactSearch or DRESModel\n",
    "        retriever = DenseRetrievalExactSearch(DRESModel(retriever_model))\n",
    "        \n",
    "        # Initialize RetrievalEvaluator with the retriever\n",
    "        self.evaluator = RetrievalEvaluator(retriever=retriever)\n",
    "\n",
    "    def evaluate(self, qs, ps):\n",
    "        if self.is_multi_vector:\n",
    "            scores = self.evaluate_colbert(qs, ps)\n",
    "        else:\n",
    "            scores = self.evaluate_biencoder(qs, ps)\n",
    "\n",
    "        assert scores.shape[0] == len(qs)\n",
    "\n",
    "        arg_score = scores.argmax(dim=1)\n",
    "        accuracy = (arg_score == torch.arange(scores.shape[0], device=scores.device)).sum().item() / scores.shape[0]\n",
    "        print(arg_score)\n",
    "        print(f\"Top 1 Accuracy (verif): {accuracy}\")\n",
    "\n",
    "        scores = scores.to(torch.float32).cpu().numpy()\n",
    "        return scores\n",
    "\n",
    "    def evaluate_colbert(self, qs, ps, batch_size=128) -> torch.Tensor:\n",
    "        scores = []\n",
    "        for i in range(0, len(qs), batch_size):\n",
    "            scores_batch = []\n",
    "            qs_batch = torch.nn.utils.rnn.pad_sequence(qs[i : i + batch_size], batch_first=True, padding_value=0).to(\n",
    "                self.device\n",
    "            )\n",
    "            for j in range(0, len(ps), batch_size):\n",
    "                ps_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "                    ps[j : j + batch_size], batch_first=True, padding_value=0\n",
    "                ).to(self.device)\n",
    "                scores_batch.append(torch.einsum(\"bnd,csd->bcns\", qs_batch, ps_batch).max(dim=3)[0].sum(dim=2))\n",
    "            scores_batch = torch.cat(scores_batch, dim=1).cpu()\n",
    "            scores.append(scores_batch)\n",
    "        scores = torch.cat(scores, dim=0)\n",
    "        return scores\n",
    "\n",
    "    def evaluate_biencoder(self, qs, ps) -> torch.Tensor:\n",
    "        qs = torch.stack(qs)\n",
    "        ps = torch.stack(ps)\n",
    "        scores = torch.einsum(\"bd,cd->bc\", qs, ps)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "Some weights of ColPali were not initialized from the model checkpoint at google/paligemma-3b-mix-448 and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\181930-Prompt-Milk-Analyzer-Brochure-12th-November-2019-for-web-and-mobile-only.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\A5-8.3_x_5.8_inch_ThawEasy_Lite.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\A5-8.3_x_5.8_inch_ThawEasy_Lite_Plus.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\A5-8.3_x_5.8_inch_ThawEasy_Smart.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\A5-8.3_x_5.8_inch_ThawEasy_Smart_Pro.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\AMCS-brochure-v2.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\AMCS-Brochure.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\AMCS_Client_Software_-_User_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Amul-Logisitic-App.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\AmulAMCS_Presentation_Final.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\AMUL_AMCS_-_Service_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Amul_AMCS_.Net_Client_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Amul_AMCS_Server_Portal_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Balinee_AMCS_Server_-_Operational_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BMC-Smartbox.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BMC_Alerts_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BMC_Configuration_Manual_-_Eng.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BMC_Smart_Box_-_Service_Manual_-_Eng.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BMC_Web_Portal_User_Manual_-_Eng.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BovEasy.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BovTag-Brochure.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\BreedEasy.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Cleaning_Procedure.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Cloud_Calibration_Server_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Configuration_Manual_Milk_Sangrah_App-English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\DMT_User_Manaual_Gujarati.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\DPMCU.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\DPMCU_RMRD_Portal_-_User_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\DPMCU_Service_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Easyline.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Farm-365.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Farm-level.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Farm365_App._User_Manual_V-1.0.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Farm365_App._User_Manual_V-1.1.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Fatomatic-Brochure.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Fatomatic___Milkofat_Service_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Installation_procedure_3_part_assembly_13042021.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\ismart.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\ismart_Split_Bitmap_compressed.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Milk-Sangrah-Brochure-.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Milkeasy-Milking-Machine-for-Web.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkEasy.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\milking-machine-catalogue.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Milking-machine-Part-.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Milking_Machine_Service_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Milking_Machine_User_Manual_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Milkocheck.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoCheck_Lab_User_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoCheck_Operator_User_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoCheck_Server_-_User_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoChill.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoChill_Installation_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoChill_Site_Suvey_Form.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MilkoChill_User_Manual-English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MIlkolite-Combo.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MIlkolite.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MPDPU_6.X.X.X_-_Installation_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MPDPU_6.X.X.X_-_Operational_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MPDPU_7.X.X.X_-_Installation_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\MPDPU_7.X.X.X_-_Operational_Manual-English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Old_.net_AMCS_Client_Software_-_User_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\PDP_Client_Service_Engineer_Manual_Analyzer.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\PDP_Client_Service_Engineer_Manual_Fatomatic.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Prompt-iPrinter_Service_Engineer_Manual_-_1.0.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Prompt-iPrinter_User_Manual_-_1.0.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\PromptAMCS_Installation_-_Final.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\PromptAMCS_RMRD_Portal_-_User_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Prompt_AMCS_.net_Client_Software_–_User_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Prompt_AMCS_Dot_net_installation_-_Hindi.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\prompt_brochure_BMC-SMartboxLR-TECHNICAL-SPECIFICATIONS.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Prompt_SmartBox_-_Installation_Manual_-_For_Customer.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Promt-Fatomatic-Brochure-with-cloud-TECHNICAL-SPECIFICATIONS.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\RMRD_Client_Software_-_User_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Sanchay.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Sanchay_Settings_&_Calibration.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Sanhay_Analyzer_Service_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart-scale-brochure_31.8.21.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\SmartBox.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\SmartBox_-_Calibration_-_For_Customer.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\SmartBox_-_Survey_Form.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\SmartBox_Configuration.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Electronic_Scale.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Electronic_Scale_Manual.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Fatomatic_Details.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Milk_Analyzer_-_Calibration_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Milk_Analyzer_-_Calibration_Manual_-_Zambia.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Milk_Analyzer_1.1.1.3_-_SOP_-_1.0.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Milk_Analyzer_1.2.X.X_-_Calibration_SOP_-_English_-_1.1.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Milk_Analyzer_Cleaning_Procedure_-_Telugu.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Smart_Milk_Analyzer_User_Manual_-_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\TankAssure-brochure_30.7.21.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\TankAssure.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\TankAssure_S15_Process_SOP_English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\TankAssure_S7_Process_SOP_English_V-1.3.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Technical-Specification-.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\ThawEasy.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Thaweasy_Technical_Technical-specification.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Think-Dairy-Tech.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Tracksure.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\TrackSure_Final_Presentation.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\User_manual_Milk_Sangrah_App-English.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\User_manual_Milk_Sangrah_App-English_V-1.0.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\User_manual_Milk_Sangrah_App-English_V-1.1.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\User_manual_Milk_Sangrah_App-English_V-1.2.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\Version_Upgrade_Process_in_Milk_Analyzer.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\मिल्कोलाइट_Combo_Settings_&_Calibration.pdf\n",
      "Indexing now: C:\\Users\\gargm\\Desktop\\Projects\\BTech\\Raw_DataFiles\\मिल्कोलाइट_Settings_&_Calibration.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\transformers\\models\\siglip\\modeling_siglip.py:574: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1404])\n",
      "Top 1 Accuracy (verif): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 76, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\grpc\\_channel.py\", line 1181, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\grpc\\_channel.py\", line 1006, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.UNAVAILABLE\n",
      "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\", grpc_status:14, created_time:\"2024-10-31T21:45:09.4283907+00:00\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 120, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ServiceUnavailable: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2018, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1567, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gargm\\AppData\\Local\\Temp\\ipykernel_69696\\1920178859.py\", line 100, in gradio_response\n",
      "    answer, image, index = answer_query(query, prompt)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gargm\\AppData\\Local\\Temp\\ipykernel_69696\\1920178859.py\", line 95, in answer_query\n",
      "    answer = f\"Gemini Response:\\n {get_answer(prompt, best_image)}\"\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gargm\\AppData\\Local\\Temp\\ipykernel_69696\\1920178859.py\", line 89, in get_answer\n",
      "    response = model.generate_content([prompt, image])\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 331, in generate_content\n",
      "    response = self._client.generate_content(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 830, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 221, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "google.api_core.exceptions.RetryError: Timeout of 600.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1404])\n",
      "Top 1 Accuracy (verif): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 76, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\grpc\\_channel.py\", line 1181, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\grpc\\_channel.py\", line 1006, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.UNAVAILABLE\n",
      "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\", grpc_status:14, created_time:\"2024-10-31T21:55:07.8348429+00:00\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 120, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ServiceUnavailable: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2018, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1567, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gargm\\AppData\\Local\\Temp\\ipykernel_69696\\1920178859.py\", line 100, in gradio_response\n",
      "    answer, image, index = answer_query(query, prompt)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gargm\\AppData\\Local\\Temp\\ipykernel_69696\\1920178859.py\", line 95, in answer_query\n",
      "    answer = f\"Gemini Response:\\n {get_answer(prompt, best_image)}\"\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\gargm\\AppData\\Local\\Temp\\ipykernel_69696\\1920178859.py\", line 89, in get_answer\n",
      "    response = model.generate_content([prompt, image])\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 331, in generate_content\n",
      "    response = self._client.generate_content(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 830, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 221, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "google.api_core.exceptions.RetryError: Timeout of 600.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.194.42:443: tcp handshaker shutdown\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PIL import Image\n",
    "from typing import Tuple, List\n",
    "import google.generativeai as genai\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.utils.colpali_processing_utils import process_queries, process_images\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig\n",
    "from pdf2image import convert_from_path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the API key from the .env file\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"vidore/colpali\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load RAG model\n",
    "retrieval_model = ColPali.from_pretrained(\"google/paligemma-3b-mix-448\",\n",
    "                                          torch_dtype=torch.float16,\n",
    "                                          device_map=\"cuda\",\n",
    "                                          quantization_config=bnb_config).eval()\n",
    "retrieval_model.load_adapter(model_name)\n",
    "paligemma_processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = retrieval_model.device\n",
    "\n",
    "# Define index function to process PDFs\n",
    "def index(files: List[str]) -> Tuple[List[torch.Tensor], List[Image.Image]]:\n",
    "    poppler_path = r\"C:\\Poppler\\poppler-24.08.0\\Library\\bin\"\n",
    "    images = []\n",
    "    document_embeddings = []\n",
    "\n",
    "    # Convert PDF pages to images\n",
    "    for file in files:\n",
    "        print(f\"Indexing now: {file}\")\n",
    "        images.extend(convert_from_path(file, poppler_path=poppler_path))\n",
    "\n",
    "    # Create DataLoader for images\n",
    "    dataloader = DataLoader(\n",
    "        images,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: process_images(paligemma_processor, x),\n",
    "    )\n",
    "\n",
    "    # Generate embeddings for each image batch\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            embeddings = retrieval_model(**batch)\n",
    "        document_embeddings.extend(list(torch.unbind(embeddings.to(\"cpu\"))))\n",
    "    \n",
    "    return document_embeddings, images\n",
    "\n",
    "# Load document files and index them\n",
    "DATA_FOLDER = \"C:\\\\Users\\\\gargm\\\\Desktop\\\\Projects\\\\BTech\\\\Raw_DataFiles\"\n",
    "pdf_files = [os.path.join(DATA_FOLDER, file) for file in os.listdir(DATA_FOLDER) if file.lower().endswith('.pdf')]\n",
    "document_embeddings, images = index(pdf_files)\n",
    "\n",
    "# Function to retrieve the top document\n",
    "def retrieve_top_document(query: str, document_embeddings: List[torch.Tensor], document_images: List[Image.Image]) -> Tuple[Image.Image, int]:\n",
    "    placeholder_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_batch = process_queries(paligemma_processor, [query], placeholder_image)\n",
    "        query_batch = {key: value.to(device) for key, value in query_batch.items()}\n",
    "        query_embeddings_tensor = retrieval_model(**query_batch)\n",
    "        query_embeddings = list(torch.unbind(query_embeddings_tensor.to(\"cpu\")))\n",
    "\n",
    "    retriever_evaluator = MyEvaluatorWrapper(is_multi_vector=True)\n",
    "    similarity_scores = retriever_evaluator.evaluate(query_embeddings, document_embeddings)\n",
    "    best_index = int(similarity_scores.argmax(axis=1).item())\n",
    "    \n",
    "    return document_images[best_index], best_index\n",
    "\n",
    "# Gemini answer generation\n",
    "def get_answer(prompt: str, image: Image) -> str:\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "    response = model.generate_content([prompt, image])\n",
    "    return response.text\n",
    "\n",
    "# Main function to handle query and prompt\n",
    "def answer_query(query: str, prompt: str) -> Tuple[str, Image.Image, int]:\n",
    "    best_image, best_index = retrieve_top_document(query, document_embeddings, images)\n",
    "    answer = f\"Gemini Response:\\n {get_answer(prompt, best_image)}\"\n",
    "    return answer, best_image, best_index\n",
    "\n",
    "# Gradio response function\n",
    "def gradio_response(query, prompt):\n",
    "    answer, image, index = answer_query(query, prompt)\n",
    "    return answer, image\n",
    "\n",
    "# Define Gradio Interface\n",
    "def chatbot_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Chatbot Name - RAG & Gemini Integration\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                query_box = gr.Textbox(label=\"Search Query\", placeholder=\"Enter search query\", lines=1)\n",
    "                prompt_box = gr.Textbox(label=\"Question\", placeholder=\"Enter question for the document\", lines=2)\n",
    "                submit_button = gr.Button(\"Submit\")\n",
    "                answer_output = gr.Textbox(label=\"Response\")\n",
    "                image_output = gr.Image(label=\"Retrieved Image\")\n",
    "\n",
    "                # Link Gradio inputs and outputs to the function\n",
    "                submit_button.click(\n",
    "                    fn=gradio_response,\n",
    "                    inputs=[query_box, prompt_box],\n",
    "                    outputs=[answer_output, image_output]\n",
    "                )\n",
    "\n",
    "        demo.launch()\n",
    "\n",
    "chatbot_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.13s/it]\n",
      "Some weights of ColPali were not initialized from the model checkpoint at google/paligemma-3b-mix-448 and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from pdf2image import convert_from_path\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig\n",
    "import google.generativeai as genai\n",
    "from torch.utils.data import DataLoader\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.utils.colpali_processing_utils import process_images, process_queries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from mteb.evaluation.evaluators import RetrievalEvaluator\n",
    "from mteb.evaluation.evaluators.RetrievalEvaluator import DenseRetrievalExactSearch, DRESModel\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"vidore/colpali\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "# Load RAG model and processor\n",
    "retrieval_model = ColPali.from_pretrained(\"google/paligemma-3b-mix-448\", torch_dtype=torch.float16, \n",
    "                                          device_map=\"cuda\", quantization_config=bnb_config).eval()\n",
    "retrieval_model.load_adapter(model_name)\n",
    "paligemma_processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = retrieval_model.device\n",
    "\n",
    "# Function to process and cache embeddings and images efficiently\n",
    "def index_optimized(files: List[str], embedding_cache: str = 'document_embeddings_cache.pt', image_dir: str = 'cached_images') -> Tuple[List[torch.Tensor], List[str]]:\n",
    "    # Check for cached embeddings and images\n",
    "    if os.path.exists(embedding_cache) and os.path.isdir(image_dir):\n",
    "        document_embeddings = torch.load(embedding_cache)\n",
    "        image_paths = [os.path.join(image_dir, img) for img in sorted(os.listdir(image_dir))]\n",
    "        return document_embeddings, image_paths\n",
    "\n",
    "    # Initialize lists for embeddings and create the image directory\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    document_embeddings = []\n",
    "    poppler_path = r\"C:\\Poppler\\poppler-24.08.0\\Library\\bin\"  # Adjust this path as needed\n",
    "    image_paths = []\n",
    "    \n",
    "    # Convert PDF pages to images and save each image separately\n",
    "    for file in files:\n",
    "        images = convert_from_path(file, poppler_path=poppler_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image_path = os.path.join(image_dir, f\"{os.path.basename(file)}_page_{i}.png\")\n",
    "            image.save(image_path, \"PNG\")\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    # Process images in batches for embedding generation\n",
    "    dataloader = DataLoader([Image.open(p) for p in image_paths], batch_size=4, shuffle=False, collate_fn=lambda x: process_images(paligemma_processor, x))\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            embeddings = retrieval_model(**batch).to(\"cpu\").float()  # Convert to float32 for compatibility\n",
    "            document_embeddings.extend(list(torch.unbind(embeddings)))\n",
    "\n",
    "    # Save embeddings to cache\n",
    "    torch.save(document_embeddings, embedding_cache)\n",
    "    return document_embeddings, image_paths\n",
    "\n",
    "# Load PDF files and index them\n",
    "DATA_FOLDER = \"C:\\\\Users\\\\gargm\\\\Desktop\\\\Projects\\\\BTech\\\\Raw_DataFiles\"\n",
    "pdf_files = [os.path.join(DATA_FOLDER, file) for file in os.listdir(DATA_FOLDER) if file.lower().endswith('.pdf')]\n",
    "document_embeddings, image_paths = index_optimized(pdf_files)\n",
    "\n",
    "# Wrapper for evaluating similarity\n",
    "class MyEvaluatorWrapper:\n",
    "    def __init__(self, retriever_model_name='all-MiniLM-L6-v2'):\n",
    "        self.device = device\n",
    "        retriever_model = SentenceTransformer(retriever_model_name)\n",
    "        retriever = DenseRetrievalExactSearch(DRESModel(retriever_model))\n",
    "        self.evaluator = RetrievalEvaluator(retriever=retriever)\n",
    "\n",
    "    def evaluate(self, qs, ps):\n",
    "        # Flatten embeddings to ensure they are 2D\n",
    "        qs = [q.view(-1, q.shape[-1]).mean(dim=0) if q.dim() > 1 else q for q in qs]\n",
    "        ps = [p.view(-1, p.shape[-1]).mean(dim=0) if p.dim() > 1 else p for p in ps]\n",
    "\n",
    "        # Stack the embeddings and convert them to float32 for compatibility\n",
    "        qs, ps = torch.stack(qs).to(torch.float32), torch.stack(ps).to(torch.float32)\n",
    "        \n",
    "        # Ensure both tensors are 2D before applying einsum\n",
    "        assert qs.dim() == 2 and ps.dim() == 2, f\"Expected 2D tensors but got {qs.dim()}D for qs and {ps.dim()}D for ps\"\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        return torch.einsum(\"bd,cd->bc\", qs, ps)\n",
    "\n",
    "# Retrieve the best matching document\n",
    "def retrieve_top_document(query: str, document_embeddings: List[torch.Tensor]) -> Tuple[str, int]:\n",
    "    placeholder_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))  # Placeholder image for query\n",
    "    with torch.no_grad():\n",
    "        query_batch = process_images(paligemma_processor, [placeholder_image])\n",
    "        query_embeddings_tensor = retrieval_model(**{k: v.to(device) for k, v in query_batch.items()}).to(\"cpu\").float()\n",
    "        query_embeddings = list(torch.unbind(query_embeddings_tensor))\n",
    "\n",
    "    # Retrieve the document most similar to the query\n",
    "    retriever_evaluator = MyEvaluatorWrapper()\n",
    "    similarity_scores = retriever_evaluator.evaluate(query_embeddings, document_embeddings)\n",
    "    best_index = int(similarity_scores.argmax(dim=1).item())\n",
    "    return image_paths[best_index], best_index\n",
    "\n",
    "# Generate answer using Gemini API\n",
    "def get_answer(prompt: str, image_path: str) -> str:\n",
    "    image = Image.open(image_path)\n",
    "    response = genai.GenerativeModel(model_name=\"gemini-1.5-flash\").generate_content([prompt, image])\n",
    "    return response.text\n",
    "\n",
    "# Main query function\n",
    "def answer_query(query: str, prompt: str) -> Tuple[str, str]:\n",
    "    best_image_path, _ = retrieve_top_document(query, document_embeddings)\n",
    "    answer = f\"Generated Response:\\n{get_answer(prompt, best_image_path)}\"\n",
    "    return answer, best_image_path\n",
    "\n",
    "# Gradio response function\n",
    "def gradio_response(query, prompt):\n",
    "    return answer_query(query, prompt)\n",
    "\n",
    "# Gradio Interface\n",
    "def chatbot_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Chatbot\")\n",
    "        with gr.Row():\n",
    "            query_box = gr.Textbox(label=\"Search Query\", placeholder=\"Enter the product name: \", lines=1)\n",
    "            prompt_box = gr.Textbox(label=\"Question\", placeholder=\"What is the issue? \", lines=2)\n",
    "            answer_output = gr.Textbox(label=\"Response\")\n",
    "            image_output = gr.Image(label=\"Retrieved Image\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            submit_button.click(fn=gradio_response, inputs=[query_box, prompt_box], outputs=[answer_output, image_output])\n",
    "        demo.launch()\n",
    "\n",
    "# Launch Gradio interface\n",
    "chatbot_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.69s/it]\n",
      "Some weights of ColPali were not initialized from the model checkpoint at google/paligemma-3b-mix-448 and are newly initialized: ['custom_text_proj.bias', 'custom_text_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 0.7 will cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 3.3 will cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2028, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1836, in postprocess_data\n",
      "    outputs_cached = await processing_utils.async_move_files_to_cache(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py\", line 637, in async_move_files_to_cache\n",
      "    return await client_utils.async_traverse(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1026, in async_traverse\n",
      "    new_obj.append(await async_traverse(item, func, is_root))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1026, in async_traverse\n",
      "    new_obj.append(await async_traverse(item, func, is_root))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1021, in async_traverse\n",
      "    new_obj[key] = await async_traverse(value, func, is_root)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1017, in async_traverse\n",
      "    return await func(json_obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py\", line 610, in _move_to_cache\n",
      "    temp_file_path = await block.async_move_resource_to_block_cache(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 312, in async_move_resource_to_block_cache\n",
      "    temp_file_path = processing_utils.save_file_to_cache(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py\", line 277, in save_file_to_cache\n",
      "    temp_dir = hash_file(file_path)\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gargm\\Desktop\\Projects\\BTech\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py\", line 206, in hash_file\n",
      "    with open(file_path, \"rb\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 22] Invalid argument: 'c:\\\\Users\\\\gargm\\\\Desktop\\\\Projects\\\\BTech\\\\Approach6\\\\The document doesn\\'t mention anything about straws. The provided document appears to be a technical specification for a product named \"MILKOFAT\". It specifies various aspects of the product like operating voltage, power consumption, capacity, etc. \\n'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from pdf2image import convert_from_path\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig\n",
    "import google.generativeai as genai\n",
    "from torch.utils.data import DataLoader\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.utils.colpali_processing_utils import process_images\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from mteb.evaluation.evaluators import RetrievalEvaluator\n",
    "from mteb.evaluation.evaluators.RetrievalEvaluator import DenseRetrievalExactSearch, DRESModel\n",
    "import tempfile  # To handle temporary image file saving\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"vidore/colpali\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "# Load RAG model and processor\n",
    "retrieval_model = ColPali.from_pretrained(\"google/paligemma-3b-mix-448\", torch_dtype=torch.float16, \n",
    "                                          device_map=\"cuda\", quantization_config=bnb_config).eval()\n",
    "retrieval_model.load_adapter(model_name)\n",
    "paligemma_processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = retrieval_model.device\n",
    "\n",
    "# Function for loading or generating embeddings\n",
    "def load_or_generate_embeddings(files: List[str], embedding_cache: str = 'document_embeddings_cache.pt', image_dir: str = 'cached_images') -> Tuple[List[torch.Tensor], List[str]]:\n",
    "    if os.path.exists(embedding_cache) and os.path.isdir(image_dir):\n",
    "        document_embeddings = torch.load(embedding_cache)\n",
    "        image_paths = [os.path.join(image_dir, img) for img in sorted(os.listdir(image_dir))]\n",
    "        return document_embeddings, image_paths\n",
    "\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    document_embeddings = []\n",
    "    poppler_path = r\"C:\\Poppler\\poppler-24.08.0\\Library\\bin\"\n",
    "    image_paths = []\n",
    "    \n",
    "    for file in files:\n",
    "        images = convert_from_path(file, poppler_path=poppler_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image_path = os.path.join(image_dir, f\"{os.path.basename(file)}_page_{i}.png\")\n",
    "            image.save(image_path, \"PNG\")\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    dataloader = DataLoader([Image.open(p) for p in image_paths], batch_size=1, shuffle=False, collate_fn=lambda x: process_images(paligemma_processor, x))\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            embeddings = retrieval_model(**batch).to(\"cpu\").float()\n",
    "            document_embeddings.extend(list(torch.unbind(embeddings)))\n",
    "\n",
    "    torch.save(document_embeddings, embedding_cache)\n",
    "    return document_embeddings, image_paths\n",
    "\n",
    "# Load PDF files and index them\n",
    "DATA_FOLDER = \"C:\\\\Users\\\\gargm\\\\Desktop\\\\Projects\\\\BTech\\\\Raw_DataFiles\"\n",
    "pdf_files = [os.path.join(DATA_FOLDER, file) for file in os.listdir(DATA_FOLDER) if file.lower().endswith('.pdf')]\n",
    "document_embeddings, image_paths = load_or_generate_embeddings(pdf_files)\n",
    "\n",
    "# Wrapper for evaluating similarity\n",
    "class MyEvaluatorWrapper:\n",
    "    def __init__(self, retriever_model_name='all-MiniLM-L6-v2'):\n",
    "        self.device = device\n",
    "        retriever_model = SentenceTransformer(retriever_model_name)\n",
    "        retriever = DenseRetrievalExactSearch(DRESModel(retriever_model))\n",
    "        self.evaluator = RetrievalEvaluator(retriever=retriever)\n",
    "\n",
    "    def evaluate(self, qs, ps):\n",
    "        qs = [q.view(-1, q.shape[-1]).mean(dim=0) if q.dim() > 1 else q for q in qs]\n",
    "        ps = [p.view(-1, p.shape[-1]).mean(dim=0) if p.dim() > 1 else p for p in ps]\n",
    "        qs, ps = torch.stack(qs).to(torch.float32), torch.stack(ps).to(torch.float32)\n",
    "        return torch.einsum(\"bd,cd->bc\", qs, ps)\n",
    "\n",
    "# Retrieve the best matching document\n",
    "def retrieve_top_document(query: str, document_embeddings: List[torch.Tensor]) -> Tuple[str, int]:\n",
    "    placeholder_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\n",
    "    with torch.no_grad():\n",
    "        query_batch = process_images(paligemma_processor, [placeholder_image])\n",
    "        query_embeddings_tensor = retrieval_model(**{k: v.to(device) for k, v in query_batch.items()})\n",
    "        query_embeddings = list(torch.unbind(query_embeddings_tensor.to(\"cpu\")))\n",
    "\n",
    "    retriever_evaluator = MyEvaluatorWrapper()\n",
    "    similarity_scores = retriever_evaluator.evaluate(query_embeddings, document_embeddings)\n",
    "    best_index = int(similarity_scores.argmax(dim=1).item())\n",
    "    return image_paths[best_index], best_index\n",
    "\n",
    "# Generate answer using Gemini API\n",
    "def get_answer(prompt: str, image_path: str) -> str:\n",
    "    image = Image.open(image_path)\n",
    "    response = genai.GenerativeModel(model_name=\"gemini-1.5-flash\").generate_content([prompt, image])\n",
    "    return response.text\n",
    "\n",
    "# Main query function\n",
    "def answer_query(query: str, prompt: str) -> Tuple[str, str]:\n",
    "    best_image_path, _ = retrieve_top_document(query, document_embeddings)\n",
    "    answer = get_answer(prompt, best_image_path)\n",
    "    return answer, best_image_path\n",
    "\n",
    "# Gradio response function for chat-style output\n",
    "def gradio_response(history, user_input):\n",
    "    query, prompt = history[-1] if history else (\"\", user_input)\n",
    "    answer, image_path = answer_query(query, prompt)\n",
    "\n",
    "    # Save the retrieved image to a temporary file\n",
    "    with Image.open(image_path) as img:\n",
    "        temp_img_path = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False).name\n",
    "        img.save(temp_img_path)\n",
    "\n",
    "    # Append both text and image path to the chat history\n",
    "    history.append((user_input, (answer, temp_img_path)))\n",
    "    return history, \"\"\n",
    "\n",
    "# Gradio Interface with custom styling\n",
    "def chatbot_interface():\n",
    "    with gr.Blocks(css=\"\"\"\n",
    "        body {background-color: #2a2866;}\n",
    "        .gradio-container {background-color: #2a2866;}\n",
    "        #chatbot {background-color: #c7e6f8; padding: 10px; border-radius: 8px;}\n",
    "        #chatbot .message {\n",
    "            background-color: #808080;\n",
    "            color: white;\n",
    "            border-radius: 5px;\n",
    "            padding: 8px;\n",
    "            margin: 5px 0;\n",
    "        }\n",
    "        #input_box textarea {\n",
    "            background-color: #c7e6f8;\n",
    "            color: black;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            padding: 8px;\n",
    "        }\n",
    "        .gr-button.new-chat {\n",
    "            color: white;\n",
    "            border: 1px solid white;\n",
    "            padding: 8px;\n",
    "            background: none;\n",
    "        }\n",
    "        .history-button {\n",
    "            background: none;\n",
    "            border: none;\n",
    "            color: white;\n",
    "            font-size: 14px;\n",
    "            text-align: left;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            gap: 8px;\n",
    "            cursor: pointer;\n",
    "            justify-content: flex-start;\n",
    "        }\n",
    "        .history-button::before {\n",
    "            content: \"\\\\1F5E8\";\n",
    "            margin-right: 5px;\n",
    "        }\n",
    "        .history-button:hover {\n",
    "            color: #c7e6f8;\n",
    "        }\n",
    "        #send_button {\n",
    "            width: 36px;\n",
    "            height: 36px;\n",
    "            background-color: #c7e6f8;\n",
    "            color: black;\n",
    "            border: none;\n",
    "            font-size: 16px;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            justify-content: center;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "    \"\"\") as demo:\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\"# Chatbot Interface\", elem_id=\"chatbot-title\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=0.7):\n",
    "                gr.Button(\"+ New chat\", elem_classes=[\"new-chat\"])\n",
    "                for i in range(3):\n",
    "                    gr.Button(f\"Chat {i+1}\", elem_classes=[\"history-button\"])\n",
    "\n",
    "            with gr.Column(scale=3.3):\n",
    "                chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "                with gr.Row():\n",
    "                    input_box = gr.Textbox(show_label=False, placeholder=\"Type your question here\", lines=1, elem_id=\"input_box\")\n",
    "                    send_button = gr.Button(\"\\u27A4\", elem_id=\"send_button\")\n",
    "\n",
    "                    send_button.click(fn=gradio_response, inputs=[chatbot, input_box], outputs=[chatbot, input_box])\n",
    "                    input_box.submit(fn=gradio_response, inputs=[chatbot, input_box], outputs=[chatbot, input_box])\n",
    "\n",
    "    demo.launch()\n",
    "\n",
    "# Launch Gradio interface\n",
    "chatbot_interface()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
